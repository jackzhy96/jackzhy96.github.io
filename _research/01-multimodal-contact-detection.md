---
title: "Multi-Modal Data Collection and Analysis for Surgical Robotics"
excerpt: "Multi-Modal Time-Synchronized Data Collection and Post-Processing Framework for Surgical Robotics, along with datasets and AI-driven applications (will update later)"
collection: research
order: 1
year: 2025
year_end: 2026
venue: "Johns Hopkins University"
location: "Baltimore, MD"
tags:
  - Deep Learning
  - Multi-Modal
  - Surgical Robotics
  - dVRK

# Add your image or GIF here:


# Publications associated with this research
publications:
  - title: "SurgSync: Time-Synchronized Multi-modal Data Collection Framework and Dataset for Surgical Robotics"
    authors: "Zhou, H.*, Liu, C.*, Wu, Y., ..., Kazanzides, P."
    venue: "IEEE Intl. Conf. on Robotics and Automation (ICRA)"
    year: 2026
    website_url: "coming_soon"
    arxiv_url: "coming_soon"
    code_url: "coming_soon"
    dataset_url: "coming_soon"
    under_review: true
    teaser: "research/test_1.png"
---

## Overview

This research represents Multi-Modal Time-Synchronized Data Collection and Post-Processing Framework for Surgical Robotics, along with datasets and AI-driven applications (e.g. tool-tissue contact detection).

## Key Contributions
- Investigate surgical robot tool-tissue contact detection using a multi-modal deep learning approach combining Multi-Scale Vision Transformer (ViT) and Mamba architectures.
- Lead and manage 8-person cross-functional team, delivering IRB-compliant data collection and analysis that accelerated publication timeline and demo readiness
- Build time-synchronized multi-modal data collection pipeline (vision + kinematics + sensor) and large (100+ trajectories) ex-vivo dataset
- Enable a novel kinematic projection approach and downstream optical flow/depth estimation using deep learning
- Design and implement a custom capacitive contact sensor to obtain the ground truth of tool-tissue contact
- Integrate a modern endoscope with the dVRK seamlessly, enabling high-quality image data acquisition
- Design a novel data annotation application with graphic user interface using PyQt for manual label annotations
- The data collection pipeline has been employed for efforts on Open-H-Embodiment

<!-- Add your media content below -->
<!-- Example for adding an image: -->
<!-- ![Research Overview](/images/research/multimodal-contact.png) -->
## SurgSync Overview
<figure>
  <img src="/images/research/test_1.png" alt="Description" style="width: 85%">
  <figcaption>SurgSync Overview</figcaption>
</figure>


## test 1
<figure>
  <div class="video-container local-video" style="width: 85%">
    <video controls playsinline>
      <source src="/images/research/test_1.mp4" type="video/mp4">
    </video>
  </div>
  <figcaption>test video 1</figcaption>
</figure>

## test 2 
<figure>
  <div class="video-container youtube-video" style="width: 85%">
    <iframe src="https://www.youtube.com/embed/dQw4w9WgXcQ" allowfullscreen></iframe>
  </div>
  <figcaption>test video 2 (embedded, it may fail)</figcaption>
</figure>

## test 3
<figure>                                                                      
    <a href="https://www.youtube.com/watch?v=x3bDhtuC5yk" target="_blank">      
      <img src="https://img.youtube.com/vi/x3bDhtuC5yk/maxresdefault.jpg" alt="Video thumbnail" style="width: 85%;">                                    
    </a>                                                                        
    <figcaption>test 3</figcaption>                          
</figure> 

<!-- Example for adding a GIF: -->
<!-- ![Demo GIF](/images/research/multimodal-contact-demo.gif) -->
